Index: create_dataset_files.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>import numpy as np\r\n\r\n\r\ndef getID(folder='data/umls/'):\r\n    lstEnts = {}\r\n    lstRels = {}\r\n    with open(folder + 'train.txt') as f, open(folder + 'train_marked.txt', 'w') as f2:\r\n        count = 0\r\n        for line in f:\r\n            line = line.strip().split()\r\n            line = [i.strip() for i in line]\r\n            # print(line[0], line[1], line[2])\r\n            if line[0] not in lstEnts:\r\n                lstEnts[line[0]] = len(lstEnts)\r\n            if line[1] not in lstRels:\r\n                lstRels[line[1]] = len(lstRels)\r\n            if line[2] not in lstEnts:\r\n                lstEnts[line[2]] = len(lstEnts)\r\n            count += 1\r\n            f2.write(str(line[0]) + '\\t' + str(line[1]) +\r\n                     '\\t' + str(line[2]) + '\\n')\r\n        print(\"Size of train_marked set set \", count)\r\n\r\n    with open(folder + 'valid.txt') as f, open(folder + 'valid_marked.txt', 'w') as f2:\r\n        count = 0\r\n        for line in f:\r\n            line = line.strip().split()\r\n            line = [i.strip() for i in line]\r\n            # print(line[0], line[1], line[2])\r\n            if line[0] not in lstEnts:\r\n                lstEnts[line[0]] = len(lstEnts)\r\n            if line[1] not in lstRels:\r\n                lstRels[line[1]] = len(lstRels)\r\n            if line[2] not in lstEnts:\r\n                lstEnts[line[2]] = len(lstEnts)\r\n            count += 1\r\n            f2.write(str(line[0]) + '\\t' + str(line[1]) +\r\n                     '\\t' + str(line[2]) + '\\n')\r\n        print(\"Size of VALID_marked set set \", count)\r\n\r\n    with open(folder + 'test.txt') as f, open(folder + 'test_marked.txt', 'w') as f2:\r\n        count = 0\r\n        for line in f:\r\n            line = line.strip().split()\r\n            line = [i.strip() for i in line]\r\n            # print(line[0], line[1], line[2])\r\n            if line[0] not in lstEnts:\r\n                lstEnts[line[0]] = len(lstEnts)\r\n            if line[1] not in lstRels:\r\n                lstRels[line[1]] = len(lstRels)\r\n            if line[2] not in lstEnts:\r\n                lstEnts[line[2]] = len(lstEnts)\r\n            count += 1\r\n            f2.write(str(line[0]) + '\\t' + str(line[1]) +\r\n                     '\\t' + str(line[2]) + '\\n')\r\n        print(\"Size of test_marked set set \", count)\r\n\r\n    wri = open(folder + 'entity2id.txt', 'w')\r\n    for entity in lstEnts:\r\n        wri.write(entity + '\\t' + str(lstEnts[entity]))\r\n        wri.write('\\n')\r\n    wri.close()\r\n\r\n    wri = open(folder + 'relation2id.txt', 'w')\r\n    for entity in lstRels:\r\n        wri.write(entity + '\\t' + str(lstRels[entity]))\r\n        wri.write('\\n')\r\n    wri.close()\r\n\r\n\r\ngetID()\r\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- create_dataset_files.py	(revision a7ee2c75c3a843e544ca707b61eb104788859d25)
+++ create_dataset_files.py	(date 1596174944840)
@@ -1,7 +1,7 @@
 import numpy as np
 
 
-def getID(folder='data/umls/'):
+def getID(folder='./data/WN18RR/'):
     lstEnts = {}
     lstRels = {}
     with open(folder + 'train.txt') as f, open(folder + 'train_marked.txt', 'w') as f2:
Index: run.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>import torch\r\n\r\nfrom models import SpKBGATModified, SpKBGATConvOnly\r\nfrom torch.autograd import Variable\r\nimport torch.nn as nn\r\nimport torch.nn.functional as F\r\nimport numpy as np\r\nimport matplotlib.pyplot as plt\r\nfrom matplotlib.lines import Line2D\r\nfrom copy import deepcopy\r\n\r\nfrom preprocess import read_entity_from_id, read_relation_from_id, init_embeddings, build_data\r\nfrom create_batch import Corpus\r\nfrom utils import save_model\r\n\r\nimport random\r\nimport argparse\r\nimport os\r\nimport sys\r\nimport logging\r\nimport time\r\nimport pickle\r\n\r\n\r\nclass Args:\r\n    # network arguments\r\n    data = \"./data/WN18RR/\"\r\n    epochs_gat = 3600\r\n    epochs_conv = 200\r\n    weight_decay_gat = float(5e-6)\r\n    weight_decay_conv = float(1e-5)\r\n    pretrained_emb = True\r\n    embedding_size = 50\r\n    lr = float(1e-3)\r\n    get_2hop = True\r\n    use_2hop = True\r\n    partial_2hop = False\r\n    output_folder = \"./checkpoints/wn/out/\"\r\n\r\n    # arguments for GAT\r\n    batch_size_gat = 86835\r\n    # Tỷ lệ của tập valid so với tập invalid trong khi training GAT\r\n    valid_invalid_ratio_gat = 2\r\n    drop_GAT = 0.3  # Tỷ lệ dropout của lớp SpGAT\r\n    alpha = 0.2  # LeakyRelu alphs for SpGAT layer\r\n    entity_out_dim = [100, 200]  # Miền nhúng của đầu ra output\r\n    nheads_GAT = [2, 2]  # Multihead attention SpGAT\r\n    # Margin used in hinge loss ( Sử dụng margin trong hinge (khớp nối))\r\n    margin = 5\r\n\r\n    # arguments for convolution network\r\n    batch_size_conv = 128  # Batch size for conv\r\n    alpha_conv = 0.2  # LeakyRelu alphas for conv layer\r\n    # Ratio of valid to invalid triples for convolution training\r\n    valid_invalid_ratio_conv = 40\r\n    out_channels = 500  # Số lượng output channels trong lớp conv\r\n    drop_conv = 0.0  # Xắc xuất dropout cho lớp convolution\r\n\r\n\r\nargs = Args()\r\n\r\nentity_embeddings = torch.load(\"./data/WN18RR/entity_embeddings.pt\")\r\nrelation_embeddings = torch.load(\"./data/WN18RR/relation_embeddings.pt\")\r\nCorpus_ = torch.load(\"./Corpus_torch.pt\")\r\n\r\nif(args.use_2hop):\r\n    print(\"Opening node_neighbors pickle object\")\r\n    file = \"./2hop.pickle\"\r\n    with open(file, 'rb') as handle:\r\n        node_neighbors_2hop = pickle.load(handle)\r\n\r\nentity_embeddings_copied = deepcopy(entity_embeddings)\r\nrelation_embeddings_copied = deepcopy(relation_embeddings)\r\n\r\n\r\nCUDA = False #torch.cuda.is_available()\r\n\r\n\r\nprint(\"Defining model\")\r\n\r\nprint(\r\n    \"\\nModel type -> GAT layer with {} heads used , Initital Embeddings training\".format(args.nheads_GAT[0]))\r\n# SpKBGATModified : lớp GAT chính\r\nmodel_gat = SpKBGATModified(entity_embeddings, relation_embeddings, args.entity_out_dim, args.entity_out_dim,\r\n                            args.drop_GAT, args.alpha, args.nheads_GAT)\r\n\r\nif CUDA:\r\n    model_gat.cuda()\r\n\r\noptimizer = torch.optim.Adam(\r\n    model_gat.parameters(), lr=args.lr, weight_decay=args.weight_decay_gat)\r\n\r\nscheduler = torch.optim.lr_scheduler.StepLR(\r\n    optimizer, step_size=500, gamma=0.5, last_epoch=-1)\r\n\r\ngat_loss_func = nn.MarginRankingLoss(margin=args.margin)\r\n\r\ncurrent_batch_2hop_indices = torch.tensor([])\r\nif(args.use_2hop):\r\n    current_batch_2hop_indices = Corpus_.get_batch_nhop_neighbors_all(args,\r\n                                                                      Corpus_.unique_entities_train, node_neighbors_2hop)\r\n\r\nif CUDA:\r\n    current_batch_2hop_indices = Variable(\r\n        torch.LongTensor(current_batch_2hop_indices)).cuda()\r\nelse:\r\n    current_batch_2hop_indices = Variable(\r\n        torch.LongTensor(current_batch_2hop_indices))\r\n\r\nepoch_losses = []   # losses of all epochs\r\nprint(\"Number of epochs {}\".format(args.epochs_gat))\r\n\r\nepoch = 1\r\n########################################################\r\ndef batch_gat_loss(gat_loss_func, train_indices, entity_embed, relation_embed):\r\n    len_pos_triples = int(\r\n        train_indices.shape[0] / (int(args.valid_invalid_ratio_gat) + 1))\r\n\r\n    pos_triples = train_indices[:len_pos_triples]\r\n    neg_triples = train_indices[len_pos_triples:]\r\n\r\n    pos_triples = pos_triples.repeat(int(args.valid_invalid_ratio_gat), 1)\r\n\r\n    source_embeds = entity_embed[pos_triples[:, 0]]\r\n    relation_embeds = relation_embed[pos_triples[:, 1]]\r\n    tail_embeds = entity_embed[pos_triples[:, 2]]\r\n\r\n    x = source_embeds + relation_embeds - tail_embeds\r\n    pos_norm = torch.norm(x, p=1, dim=1)\r\n\r\n    source_embeds = entity_embed[neg_triples[:, 0]]\r\n    relation_embeds = relation_embed[neg_triples[:, 1]]\r\n    tail_embeds = entity_embed[neg_triples[:, 2]]\r\n\r\n    x = source_embeds + relation_embeds - tail_embeds\r\n    neg_norm = torch.norm(x, p=1, dim=1)\r\n\r\n    if (CUDA):\r\n        y = torch.ones(int(args.valid_invalid_ratio_gat) * len_pos_triples).cuda()\r\n    else:\r\n        y = torch.ones(int(args.valid_invalid_ratio_gat) * len_pos_triples)\r\n    loss = gat_loss_func(pos_norm, neg_norm, y)\r\n    return loss\r\n############################################\r\n\r\n\r\nrandom.shuffle(Corpus_.train_triples)\r\nCorpus_.train_indices = np.array(\r\n    list(Corpus_.train_triples)).astype(np.int32)\r\n\r\nmodel_gat.train()  # getting in training mode\r\nstart_time = time.time()\r\nepoch_loss = []\r\n\r\nif len(Corpus_.train_indices) % args.batch_size_gat == 0:\r\n    num_iters_per_epoch = len(\r\n        Corpus_.train_indices) // args.batch_size_gat\r\nelse:\r\n    num_iters_per_epoch = (\r\n                                  len(Corpus_.train_indices) // args.batch_size_gat) + 1\r\n\r\nfor iters in range(num_iters_per_epoch):\r\n    start_time_iter = time.time()\r\n    train_indices, train_values = Corpus_.get_iteration_batch(iters)\r\n\r\n    if CUDA:\r\n        train_indices = Variable(\r\n            torch.LongTensor(train_indices)).cuda()\r\n        train_values = Variable(torch.FloatTensor(train_values)).cuda()\r\n\r\n    else:\r\n        train_indices = Variable(torch.LongTensor(train_indices))\r\n        train_values = Variable(torch.FloatTensor(train_values))\r\n\r\n    # forward pass\r\n    entity_embed, relation_embed = model_gat(\r\n        Corpus_, Corpus_.train_adj_matrix, train_indices, current_batch_2hop_indices)\r\n\r\n    optimizer.zero_grad()\r\n\r\n    loss = batch_gat_loss(\r\n        gat_loss_func, train_indices, entity_embed, relation_embed)\r\n\r\n    loss.backward()\r\n    optimizer.step()\r\n\r\n    epoch_loss.append(loss.data.item())\r\n\r\n    end_time_iter = time.time()\r\n\r\n    print(\"Iteration-> {0}  , Iteration_time-> {1:.4f} , Iteration_loss {2:.4f}\".format(\r\n        iters, end_time_iter - start_time_iter, loss.data.item()))\r\n\r\nscheduler.step()\r\nprint(\"Epoch {} , average loss {} , epoch_time {}\".format(\r\n    epoch, sum(epoch_loss) / len(epoch_loss), time.time() - start_time))\r\nepoch_losses.append(sum(epoch_loss) / len(epoch_loss))\r\n\r\nsave_model(model_gat, args.data, epoch,\r\n           args.output_folder)\r\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- run.py	(revision a7ee2c75c3a843e544ca707b61eb104788859d25)
+++ run.py	(date 1596696766407)
@@ -1,26 +1,47 @@
+import pickle
+import random
+import time
+import os
+from copy import deepcopy
+
+import numpy as np
 import torch
-
-from models import SpKBGATModified, SpKBGATConvOnly
-from torch.autograd import Variable
 import torch.nn as nn
-import torch.nn.functional as F
-import numpy as np
-import matplotlib.pyplot as plt
-from matplotlib.lines import Line2D
-from copy import deepcopy
-
-from preprocess import read_entity_from_id, read_relation_from_id, init_embeddings, build_data
+from torch.autograd import Variable
+from preprocess import build_data, init_embeddings
 from create_batch import Corpus
-from utils import save_model
+
+from models import SpKBGATModified, SpKBGATConvOnly
+
+def batch_gat_loss(gat_loss_func, train_indices, entity_embed, relation_embed):
+    len_pos_triples = int(
+        train_indices.shape[0] / (int(args.valid_invalid_ratio_gat) + 1))
+
+    pos_triples = train_indices[:len_pos_triples]
+    neg_triples = train_indices[len_pos_triples:]
 
-import random
-import argparse
-import os
-import sys
-import logging
-import time
-import pickle
+    pos_triples = pos_triples.repeat(int(args.valid_invalid_ratio_gat), 1)
 
+    source_embeds = entity_embed[pos_triples[:, 0]]
+    relation_embeds = relation_embed[pos_triples[:, 1]]
+    tail_embeds = entity_embed[pos_triples[:, 2]]
+
+    x = source_embeds + relation_embeds - tail_embeds
+    pos_norm = torch.norm(x, p=1, dim=1)
+
+    source_embeds = entity_embed[neg_triples[:, 0]]
+    relation_embeds = relation_embed[neg_triples[:, 1]]
+    tail_embeds = entity_embed[neg_triples[:, 2]]
+
+    x = source_embeds + relation_embeds - tail_embeds
+    neg_norm = torch.norm(x, p=1, dim=1)
+
+    if (CUDA):
+        y = torch.ones(int(args.valid_invalid_ratio_gat) * len_pos_triples).cuda()
+    else:
+        y = torch.ones(int(args.valid_invalid_ratio_gat) * len_pos_triples)
+    loss = gat_loss_func(pos_norm, neg_norm, y)
+    return loss
 
 class Args:
     # network arguments
@@ -35,7 +56,7 @@
     get_2hop = True
     use_2hop = True
     partial_2hop = False
-    output_folder = "./checkpoints/wn/out/"
+    output_folder = "./"
 
     # arguments for GAT
     batch_size_gat = 86835
@@ -59,34 +80,55 @@
 
 args = Args()
 
+# Load dữ liệu
+train_data, validation_data, test_data, entity2id, relation2id, headTailSelector, unique_entities_train = build_data(
+    args.data, is_unweigted=False, directed=True)
+
+if args.pretrained_emb:
+    entity_embeddings, relation_embeddings = init_embeddings(os.path.join(args.data, 'entity2vec.txt'),
+                                                             os.path.join(args.data, 'relation2vec.txt'))
+    print("Initialised relations and entities from TransE")
+
+else:
+    entity_embeddings = np.random.randn(
+        len(entity2id), args.embedding_size)
+    relation_embeddings = np.random.randn(
+        len(relation2id), args.embedding_size)
+    print("Initialised relations and entities randomly")
+
+# Corpus_ = Corpus(args, train_data, validation_data, test_data, entity2id, relation2id, headTailSelector,
+#                 args.batch_size_gat, args.valid_invalid_ratio_gat, unique_entities_train, args.get_2hop)
+
+# entity_embeddings = torch.FloatTensor(entity_embeddings)
+# relation_embeddings = torch.FloatTensor(relation_embeddings)
+#####################################
 entity_embeddings = torch.load("./data/WN18RR/entity_embeddings.pt")
 relation_embeddings = torch.load("./data/WN18RR/relation_embeddings.pt")
 Corpus_ = torch.load("./Corpus_torch.pt")
 
-if(args.use_2hop):
+if (args.use_2hop):
     print("Opening node_neighbors pickle object")
     file = "./2hop.pickle"
     with open(file, 'rb') as handle:
         node_neighbors_2hop = pickle.load(handle)
 
-entity_embeddings_copied = deepcopy(entity_embeddings)
-relation_embeddings_copied = deepcopy(relation_embeddings)
-
+# entity_embeddings_copied = deepcopy(entity_embeddings)
+# relation_embeddings_copied = deepcopy(relation_embeddings)
 
-CUDA = False #torch.cuda.is_available()
+CUDA = False
 
-
+#####################################
 print("Defining model")
 
-print(
-    "\nModel type -> GAT layer with {} heads used , Initital Embeddings training".format(args.nheads_GAT[0]))
-# SpKBGATModified : lớp GAT chính
+# print(
+#     "\nModel type -> GAT layer with {} heads used , Initital Embeddings training".format(args.nheads_GAT[0]))
+# # SpKBGATModified : lớp GAT chính
 model_gat = SpKBGATModified(entity_embeddings, relation_embeddings, args.entity_out_dim, args.entity_out_dim,
                             args.drop_GAT, args.alpha, args.nheads_GAT)
-
-if CUDA:
-    model_gat.cuda()
-
+#
+# if CUDA:
+#     model_gat.cuda()
+#
 optimizer = torch.optim.Adam(
     model_gat.parameters(), lr=args.lr, weight_decay=args.weight_decay_gat)
 
@@ -96,9 +138,10 @@
 gat_loss_func = nn.MarginRankingLoss(margin=args.margin)
 
 current_batch_2hop_indices = torch.tensor([])
-if(args.use_2hop):
+if (args.use_2hop):
     current_batch_2hop_indices = Corpus_.get_batch_nhop_neighbors_all(args,
-                                                                      Corpus_.unique_entities_train, node_neighbors_2hop)
+                                                                      Corpus_.unique_entities_train,
+                                                                      node_neighbors_2hop)
 
 if CUDA:
     current_batch_2hop_indices = Variable(
@@ -107,43 +150,11 @@
     current_batch_2hop_indices = Variable(
         torch.LongTensor(current_batch_2hop_indices))
 
-epoch_losses = []   # losses of all epochs
+epoch_losses = []  # losses of all epochs
 print("Number of epochs {}".format(args.epochs_gat))
-
+#
 epoch = 1
-########################################################
-def batch_gat_loss(gat_loss_func, train_indices, entity_embed, relation_embed):
-    len_pos_triples = int(
-        train_indices.shape[0] / (int(args.valid_invalid_ratio_gat) + 1))
-
-    pos_triples = train_indices[:len_pos_triples]
-    neg_triples = train_indices[len_pos_triples:]
-
-    pos_triples = pos_triples.repeat(int(args.valid_invalid_ratio_gat), 1)
-
-    source_embeds = entity_embed[pos_triples[:, 0]]
-    relation_embeds = relation_embed[pos_triples[:, 1]]
-    tail_embeds = entity_embed[pos_triples[:, 2]]
-
-    x = source_embeds + relation_embeds - tail_embeds
-    pos_norm = torch.norm(x, p=1, dim=1)
-
-    source_embeds = entity_embed[neg_triples[:, 0]]
-    relation_embeds = relation_embed[neg_triples[:, 1]]
-    tail_embeds = entity_embed[neg_triples[:, 2]]
-
-    x = source_embeds + relation_embeds - tail_embeds
-    neg_norm = torch.norm(x, p=1, dim=1)
-
-    if (CUDA):
-        y = torch.ones(int(args.valid_invalid_ratio_gat) * len_pos_triples).cuda()
-    else:
-        y = torch.ones(int(args.valid_invalid_ratio_gat) * len_pos_triples)
-    loss = gat_loss_func(pos_norm, neg_norm, y)
-    return loss
-############################################
-
-
+# ############################################
 random.shuffle(Corpus_.train_triples)
 Corpus_.train_indices = np.array(
     list(Corpus_.train_triples)).astype(np.int32)
@@ -159,7 +170,7 @@
     num_iters_per_epoch = (
                                   len(Corpus_.train_indices) // args.batch_size_gat) + 1
 
-for iters in range(num_iters_per_epoch):
+for iters in range(1):
     start_time_iter = time.time()
     train_indices, train_values = Corpus_.get_iteration_batch(iters)
 
@@ -188,13 +199,88 @@
 
     end_time_iter = time.time()
 
-    print("Iteration-> {0}  , Iteration_time-> {1:.4f} , Iteration_loss {2:.4f}".format(
-        iters, end_time_iter - start_time_iter, loss.data.item()))
+
+######################################################################
 
-scheduler.step()
-print("Epoch {} , average loss {} , epoch_time {}".format(
-    epoch, sum(epoch_loss) / len(epoch_loss), time.time() - start_time))
-epoch_losses.append(sum(epoch_loss) / len(epoch_loss))
 
-save_model(model_gat, args.data, epoch,
-           args.output_folder)
+# print("Only Conv model trained")
+# model_conv = SpKBGATConvOnly(entity_embeddings, relation_embeddings, args.entity_out_dim, args.entity_out_dim,
+#                              args.drop_GAT, args.drop_conv, args.alpha, args.alpha_conv,
+#                              args.nheads_GAT, args.out_channels)
+#
+# if CUDA:
+#     model_conv.cuda()
+#     model_gat.cuda()
+#
+# model_gat.load_state_dict(torch.load(
+#     '{}/trained_{}.pth'.format("./gat/", args.epochs_gat - 1), map_location={'cuda:0': 'cpu'}), strict=False)
+# model_conv.final_entity_embeddings = model_gat.final_entity_embeddings
+# model_conv.final_relation_embeddings = model_gat.final_relation_embeddings
+#
+# Corpus_.batch_size = args.batch_size_conv
+# Corpus_.invalid_valid_ratio = int(args.valid_invalid_ratio_conv)
+#
+# optimizer = torch.optim.Adam(
+#     model_conv.parameters(), lr=args.lr, weight_decay=args.weight_decay_conv)
+#
+# scheduler = torch.optim.lr_scheduler.StepLR(
+#     optimizer, step_size=25, gamma=0.5, last_epoch=-1)
+#
+# margin_loss = torch.nn.SoftMarginLoss()
+#
+#
+# ############################################
+# print("\nepoch-> ", 0)
+# random.shuffle(Corpus_.train_triples)
+# Corpus_.train_indices = np.array(
+#     list(Corpus_.train_triples)).astype(np.int32)
+#
+# model_conv.train()  # getting in training mode
+# start_time = time.time()
+# epoch_loss = []
+#
+# if len(Corpus_.train_indices) % args.batch_size_conv == 0:
+#     num_iters_per_epoch = len(
+#         Corpus_.train_indices) // args.batch_size_conv
+# else:
+#     num_iters_per_epoch = (
+#         len(Corpus_.train_indices) // args.batch_size_conv) + 1
+#
+#
+# train_indices, train_values = Corpus_.get_iteration_batch(0)
+#
+# if CUDA:
+#     train_indices = Variable(
+#         torch.LongTensor(train_indices)).cuda()
+#     train_values = Variable(torch.FloatTensor(train_values)).cuda()
+#
+# else:
+#     train_indices = Variable(torch.LongTensor(train_indices))
+#     train_values = Variable(torch.FloatTensor(train_values))
+#
+# preds = model_conv(
+#     Corpus_, Corpus_.train_adj_matrix, train_indices)
+#
+# optimizer.zero_grad()
+#
+# loss = margin_loss(preds.view(-1), train_values.view(-1))
+#
+# loss.backward()
+# optimizer.step()
+
+############################################################
+# model_conv = SpKBGATConvOnly(entity_embeddings, relation_embeddings, args.entity_out_dim, args.entity_out_dim,
+#                              args.drop_GAT, args.drop_conv, args.alpha, args.alpha_conv,
+#                              args.nheads_GAT, args.out_channels)
+# model_conv.load_state_dict(torch.load(
+#     '{0}/trained_{1}.pth'.format("./conv/", args.epochs_conv - 1), map_location={'cuda:0': 'cpu'}), strict=False)
+#
+# # model_gat.load_state_dict(torch.load(
+# #     '{}/trained_{}.pth'.format("./gat/", args.epochs_gat - 1), map_location={'cuda:0': 'cpu'}), strict=False)
+#
+# if CUDA:
+#     model_conv.cuda()
+# model_conv.eval()
+# with torch.no_grad():
+#     Corpus_.get_validation_pred(args, model_conv, Corpus_.unique_entities_train)
+
Index: main.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>import torch\r\n\r\nfrom models import SpKBGATModified, SpKBGATConvOnly\r\nfrom torch.autograd import Variable\r\nimport torch.nn as nn\r\nimport torch.nn.functional as F\r\nimport numpy as np\r\nimport matplotlib.pyplot as plt\r\nfrom matplotlib.lines import Line2D\r\nfrom copy import deepcopy\r\n\r\nfrom preprocess import read_entity_from_id, read_relation_from_id, init_embeddings, build_data\r\nfrom create_batch import Corpus\r\nfrom utils import save_model\r\n\r\nimport random\r\nimport argparse\r\nimport os\r\nimport sys\r\nimport logging\r\nimport time\r\nimport pickle\r\n\r\n# %%\r\n# %%from torchviz import make_dot, make_dot_from_trace\r\n\r\n\r\ndef parse_args():\r\n    args = argparse.ArgumentParser()\r\n    # network arguments\r\n    args.add_argument(\"-data\", \"--data\",\r\n                      default=\"./data/WN18RR/\", help=\"data directory\")\r\n    args.add_argument(\"-e_g\", \"--epochs_gat\", type=int,\r\n                      default=3600, help=\"Number of epochs\")\r\n    args.add_argument(\"-e_c\", \"--epochs_conv\", type=int,\r\n                      default=200, help=\"Number of epochs\")\r\n    args.add_argument(\"-w_gat\", \"--weight_decay_gat\", type=float,\r\n                      default=5e-6, help=\"L2 reglarization for gat\")\r\n    args.add_argument(\"-w_conv\", \"--weight_decay_conv\", type=float,\r\n                      default=1e-5, help=\"L2 reglarization for conv\")\r\n    args.add_argument(\"-pre_emb\", \"--pretrained_emb\", type=bool,\r\n                      default=True, help=\"Use pretrained embeddings\")\r\n    args.add_argument(\"-emb_size\", \"--embedding_size\", type=int,\r\n                      default=50, help=\"Size of embeddings (if pretrained not used)\")\r\n    args.add_argument(\"-l\", \"--lr\", type=float, default=1e-3)\r\n    args.add_argument(\"-g2hop\", \"--get_2hop\", type=bool, default=False)\r\n    args.add_argument(\"-u2hop\", \"--use_2hop\", type=bool, default=True)\r\n    args.add_argument(\"-p2hop\", \"--partial_2hop\", type=bool, default=False)\r\n    args.add_argument(\"-outfolder\", \"--output_folder\",\r\n                      default=\"./checkpoints/wn/out/\", help=\"Folder name to save the models.\")\r\n\r\n    # arguments for GAT\r\n    args.add_argument(\"-b_gat\", \"--batch_size_gat\", type=int,\r\n                      default=86835, help=\"Batch size for GAT\")\r\n    args.add_argument(\"-neg_s_gat\", \"--valid_invalid_ratio_gat\", type=int,\r\n                      default=2, help=\"Ratio of valid to invalid triples for GAT training\")\r\n    args.add_argument(\"-drop_GAT\", \"--drop_GAT\", type=float,\r\n                      default=0.3, help=\"Dropout probability for SpGAT layer\")\r\n    args.add_argument(\"-alpha\", \"--alpha\", type=float,\r\n                      default=0.2, help=\"LeakyRelu alphs for SpGAT layer\")\r\n    args.add_argument(\"-out_dim\", \"--entity_out_dim\", type=int, nargs='+',\r\n                      default=[100, 200], help=\"Entity output embedding dimensions\")\r\n    args.add_argument(\"-h_gat\", \"--nheads_GAT\", type=int, nargs='+',\r\n                      default=[2, 2], help=\"Multihead attention SpGAT\")\r\n    args.add_argument(\"-margin\", \"--margin\", type=float,\r\n                      default=5, help=\"Margin used in hinge loss\")\r\n\r\n    # arguments for convolution network\r\n    args.add_argument(\"-b_conv\", \"--batch_size_conv\", type=int,\r\n                      default=128, help=\"Batch size for conv\")\r\n    args.add_argument(\"-alpha_conv\", \"--alpha_conv\", type=float,\r\n                      default=0.2, help=\"LeakyRelu alphas for conv layer\")\r\n    args.add_argument(\"-neg_s_conv\", \"--valid_invalid_ratio_conv\", type=int, default=40,\r\n                      help=\"Ratio of valid to invalid triples for convolution training\")\r\n    args.add_argument(\"-o\", \"--out_channels\", type=int, default=500,\r\n                      help=\"Number of output channels in conv layer\")\r\n    args.add_argument(\"-drop_conv\", \"--drop_conv\", type=float,\r\n                      default=0.0, help=\"Dropout probability for convolution layer\")\r\n\r\n    args = args.parse_args()\r\n    return args\r\n\r\n\r\nargs = parse_args()\r\n# %%\r\n\r\n\r\ndef load_data(args):\r\n    train_data, validation_data, test_data, entity2id, relation2id, headTailSelector, unique_entities_train = build_data(\r\n        args.data, is_unweigted=False, directed=True)\r\n\r\n    if args.pretrained_emb:\r\n        entity_embeddings, relation_embeddings = init_embeddings(os.path.join(args.data, 'entity2vec.txt'),\r\n                                                                 os.path.join(args.data, 'relation2vec.txt'))\r\n        print(\"Initialised relations and entities from TransE\")\r\n\r\n    else:\r\n        entity_embeddings = np.random.randn(\r\n            len(entity2id), args.embedding_size)\r\n        relation_embeddings = np.random.randn(\r\n            len(relation2id), args.embedding_size)\r\n        print(\"Initialised relations and entities randomly\")\r\n\r\n    corpus = Corpus(args, train_data, validation_data, test_data, entity2id, relation2id, headTailSelector,\r\n                    args.batch_size_gat, args.valid_invalid_ratio_gat, unique_entities_train, args.get_2hop)\r\n\r\n    return corpus, torch.FloatTensor(entity_embeddings), torch.FloatTensor(relation_embeddings)\r\n\r\n\r\nCorpus_, entity_embeddings, relation_embeddings = load_data(args)\r\n\r\n\r\nif(args.get_2hop):\r\n    file = args.data + \"/2hop.pickle\"\r\n    with open(file, 'wb') as handle:\r\n        pickle.dump(Corpus_.node_neighbors_2hop, handle,\r\n                    protocol=pickle.HIGHEST_PROTOCOL)\r\n\r\n\r\nif(args.use_2hop):\r\n    print(\"Opening node_neighbors pickle object\")\r\n    file = args.data + \"/2hop.pickle\"\r\n    with open(file, 'rb') as handle:\r\n        node_neighbors_2hop = pickle.load(handle)\r\n\r\nentity_embeddings_copied = deepcopy(entity_embeddings)\r\nrelation_embeddings_copied = deepcopy(relation_embeddings)\r\n\r\nprint(\"Initial entity dimensions {} , relation dimensions {}\".format(\r\n    entity_embeddings.size(), relation_embeddings.size()))\r\n# %%\r\n\r\nCUDA = torch.cuda.is_available()\r\n\r\n\r\ndef batch_gat_loss(gat_loss_func, train_indices, entity_embed, relation_embed):\r\n    len_pos_triples = int(\r\n        train_indices.shape[0] / (int(args.valid_invalid_ratio_gat) + 1))\r\n\r\n    pos_triples = train_indices[:len_pos_triples]\r\n    neg_triples = train_indices[len_pos_triples:]\r\n\r\n    pos_triples = pos_triples.repeat(int(args.valid_invalid_ratio_gat), 1)\r\n\r\n    source_embeds = entity_embed[pos_triples[:, 0]]\r\n    relation_embeds = relation_embed[pos_triples[:, 1]]\r\n    tail_embeds = entity_embed[pos_triples[:, 2]]\r\n\r\n    x = source_embeds + relation_embeds - tail_embeds\r\n    pos_norm = torch.norm(x, p=1, dim=1)\r\n\r\n    source_embeds = entity_embed[neg_triples[:, 0]]\r\n    relation_embeds = relation_embed[neg_triples[:, 1]]\r\n    tail_embeds = entity_embed[neg_triples[:, 2]]\r\n\r\n    x = source_embeds + relation_embeds - tail_embeds\r\n    neg_norm = torch.norm(x, p=1, dim=1)\r\n\r\n    y = torch.ones(int(args.valid_invalid_ratio_gat) * len_pos_triples).cuda()\r\n\r\n    loss = gat_loss_func(pos_norm, neg_norm, y)\r\n    return loss\r\n\r\n\r\ndef train_gat(args):\r\n\r\n    # Creating the gat model here.\r\n    ####################################\r\n\r\n    print(\"Defining model\")\r\n\r\n    print(\r\n        \"\\nModel type -> GAT layer with {} heads used , Initital Embeddings training\".format(args.nheads_GAT[0]))\r\n    # SpKBGATModified : lớp GAT chính\r\n    model_gat = SpKBGATModified(entity_embeddings, relation_embeddings, args.entity_out_dim, args.entity_out_dim,\r\n                                args.drop_GAT, args.alpha, args.nheads_GAT)\r\n\r\n    if CUDA:\r\n        model_gat.cuda()\r\n\r\n    optimizer = torch.optim.Adam(\r\n        model_gat.parameters(), lr=args.lr, weight_decay=args.weight_decay_gat)\r\n\r\n    scheduler = torch.optim.lr_scheduler.StepLR(\r\n        optimizer, step_size=500, gamma=0.5, last_epoch=-1)\r\n\r\n    gat_loss_func = nn.MarginRankingLoss(margin=args.margin)\r\n\r\n    current_batch_2hop_indices = torch.tensor([])\r\n    if(args.use_2hop):\r\n        current_batch_2hop_indices = Corpus_.get_batch_nhop_neighbors_all(args,\r\n                                                                          Corpus_.unique_entities_train, node_neighbors_2hop)\r\n\r\n    if CUDA:\r\n        current_batch_2hop_indices = Variable(\r\n            torch.LongTensor(current_batch_2hop_indices)).cuda()\r\n    else:\r\n        current_batch_2hop_indices = Variable(\r\n            torch.LongTensor(current_batch_2hop_indices))\r\n\r\n    epoch_losses = []   # losses of all epochs\r\n    print(\"Number of epochs {}\".format(args.epochs_gat))\r\n\r\n    for epoch in range(args.epochs_gat):\r\n        print(\"\\nepoch-> \", epoch)\r\n        random.shuffle(Corpus_.train_triples)\r\n        Corpus_.train_indices = np.array(\r\n            list(Corpus_.train_triples)).astype(np.int32)\r\n\r\n        model_gat.train()  # getting in training mode\r\n        start_time = time.time()\r\n        epoch_loss = []\r\n\r\n        if len(Corpus_.train_indices) % args.batch_size_gat == 0:\r\n            num_iters_per_epoch = len(\r\n                Corpus_.train_indices) // args.batch_size_gat\r\n        else:\r\n            num_iters_per_epoch = (\r\n                len(Corpus_.train_indices) // args.batch_size_gat) + 1\r\n\r\n        for iters in range(num_iters_per_epoch):\r\n            start_time_iter = time.time()\r\n            train_indices, train_values = Corpus_.get_iteration_batch(iters)\r\n\r\n            if CUDA:\r\n                train_indices = Variable(\r\n                    torch.LongTensor(train_indices)).cuda()\r\n                train_values = Variable(torch.FloatTensor(train_values)).cuda()\r\n\r\n            else:\r\n                train_indices = Variable(torch.LongTensor(train_indices))\r\n                train_values = Variable(torch.FloatTensor(train_values))\r\n\r\n            # forward pass\r\n            entity_embed, relation_embed = model_gat(\r\n                Corpus_, Corpus_.train_adj_matrix, train_indices, current_batch_2hop_indices)\r\n            \r\n\r\n            optimizer.zero_grad()\r\n\r\n            loss = batch_gat_loss(\r\n                gat_loss_func, train_indices, entity_embed, relation_embed)\r\n\r\n            loss.backward()\r\n            optimizer.step()\r\n\r\n            epoch_loss.append(loss.data.item())\r\n\r\n            end_time_iter = time.time()\r\n\r\n            print(\"Iteration-> {0}  , Iteration_time-> {1:.4f} , Iteration_loss {2:.4f}\".format(\r\n                iters, end_time_iter - start_time_iter, loss.data.item()))\r\n\r\n        scheduler.step()\r\n        print(\"Epoch {} , average loss {} , epoch_time {}\".format(\r\n            epoch, sum(epoch_loss) / len(epoch_loss), time.time() - start_time))\r\n        epoch_losses.append(sum(epoch_loss) / len(epoch_loss))\r\n\r\n        save_model(model_gat, args.data, epoch,\r\n                   args.output_folder)\r\n\r\n\r\ndef train_conv(args):\r\n\r\n    # Creating convolution model here.\r\n    ####################################\r\n\r\n    print(\"Defining model\")\r\n    model_gat = SpKBGATModified(entity_embeddings, relation_embeddings, args.entity_out_dim, args.entity_out_dim,\r\n                                args.drop_GAT, args.alpha, args.nheads_GAT)\r\n    print(\"Only Conv model trained\")\r\n    model_conv = SpKBGATConvOnly(entity_embeddings, relation_embeddings, args.entity_out_dim, args.entity_out_dim,\r\n                                 args.drop_GAT, args.drop_conv, args.alpha, args.alpha_conv,\r\n                                 args.nheads_GAT, args.out_channels)\r\n\r\n    if CUDA:\r\n        model_conv.cuda()\r\n        model_gat.cuda()\r\n\r\n    model_gat.load_state_dict(torch.load(\r\n        '{}/trained_{}.pth'.format(args.output_folder, args.epochs_gat - 1)), strict=False)\r\n    model_conv.final_entity_embeddings = model_gat.final_entity_embeddings\r\n    model_conv.final_relation_embeddings = model_gat.final_relation_embeddings\r\n\r\n    Corpus_.batch_size = args.batch_size_conv\r\n    Corpus_.invalid_valid_ratio = int(args.valid_invalid_ratio_conv)\r\n\r\n    optimizer = torch.optim.Adam(\r\n        model_conv.parameters(), lr=args.lr, weight_decay=args.weight_decay_conv)\r\n\r\n    scheduler = torch.optim.lr_scheduler.StepLR(\r\n        optimizer, step_size=25, gamma=0.5, last_epoch=-1)\r\n\r\n    margin_loss = torch.nn.SoftMarginLoss()\r\n\r\n    epoch_losses = []   # losses of all epochs\r\n    print(\"Number of epochs {}\".format(args.epochs_conv))\r\n\r\n    for epoch in range(args.epochs_conv):\r\n        print(\"\\nepoch-> \", epoch)\r\n        random.shuffle(Corpus_.train_triples)\r\n        Corpus_.train_indices = np.array(\r\n            list(Corpus_.train_triples)).astype(np.int32)\r\n\r\n        model_conv.train()  # getting in training mode\r\n        start_time = time.time()\r\n        epoch_loss = []\r\n\r\n        if len(Corpus_.train_indices) % args.batch_size_conv == 0:\r\n            num_iters_per_epoch = len(\r\n                Corpus_.train_indices) // args.batch_size_conv\r\n        else:\r\n            num_iters_per_epoch = (\r\n                len(Corpus_.train_indices) // args.batch_size_conv) + 1\r\n\r\n        for iters in range(num_iters_per_epoch):\r\n            start_time_iter = time.time()\r\n            train_indices, train_values = Corpus_.get_iteration_batch(iters)\r\n\r\n            if CUDA:\r\n                train_indices = Variable(\r\n                    torch.LongTensor(train_indices)).cuda()\r\n                train_values = Variable(torch.FloatTensor(train_values)).cuda()\r\n\r\n            else:\r\n                train_indices = Variable(torch.LongTensor(train_indices))\r\n                train_values = Variable(torch.FloatTensor(train_values))\r\n\r\n            preds = model_conv(\r\n                Corpus_, Corpus_.train_adj_matrix, train_indices)\r\n\r\n            optimizer.zero_grad()\r\n\r\n            loss = margin_loss(preds.view(-1), train_values.view(-1))\r\n\r\n            loss.backward()\r\n            optimizer.step()\r\n\r\n            epoch_loss.append(loss.data.item())\r\n\r\n            end_time_iter = time.time()\r\n\r\n            print(\"Iteration-> {0}  , Iteration_time-> {1:.4f} , Iteration_loss {2:.4f}\".format(\r\n                iters, end_time_iter - start_time_iter, loss.data.item()))\r\n\r\n        scheduler.step()\r\n        print(\"Epoch {} , average loss {} , epoch_time {}\".format(\r\n            epoch, sum(epoch_loss) / len(epoch_loss), time.time() - start_time))\r\n        epoch_losses.append(sum(epoch_loss) / len(epoch_loss))\r\n\r\n        save_model(model_conv, args.data, epoch,\r\n                   args.output_folder + \"conv/\")\r\n\r\n\r\ndef evaluate_conv(args, unique_entities):\r\n    model_conv = SpKBGATConvOnly(entity_embeddings, relation_embeddings, args.entity_out_dim, args.entity_out_dim,\r\n                                 args.drop_GAT, args.drop_conv, args.alpha, args.alpha_conv,\r\n                                 args.nheads_GAT, args.out_channels)\r\n    model_conv.load_state_dict(torch.load(\r\n        '{0}conv/trained_{1}.pth'.format(args.output_folder, args.epochs_conv - 1)), strict=False)\r\n\r\n    model_conv.cuda()\r\n    model_conv.eval()\r\n    with torch.no_grad():\r\n        Corpus_.get_validation_pred(args, model_conv, unique_entities)\r\n\r\n\r\ntrain_gat(args)\r\n\r\ntrain_conv(args)\r\nevaluate_conv(args, Corpus_.unique_entities_train)\r\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- main.py	(revision a7ee2c75c3a843e544ca707b61eb104788859d25)
+++ main.py	(date 1596795535125)
@@ -258,6 +258,7 @@
 
         save_model(model_gat, args.data, epoch,
                    args.output_folder)
+    
 
 
 def train_conv(args):
Index: create_batch.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>import torch\r\nimport numpy as np\r\nfrom collections import defaultdict\r\nimport time\r\nimport queue\r\nimport random\r\n\r\n\r\nclass Corpus:\r\n    def __init__(self, args, train_data, validation_data, test_data, entity2id,\r\n                 relation2id, headTailSelector, batch_size, valid_to_invalid_samples_ratio, unique_entities_train, get_2hop=False):\r\n        self.train_triples = train_data[0]\r\n\r\n        # Converting to sparse tensor\r\n        adj_indices = torch.LongTensor(\r\n            [train_data[1][0], train_data[1][1]])  # rows and columns\r\n        adj_values = torch.LongTensor(train_data[1][2])\r\n        self.train_adj_matrix = (adj_indices, adj_values)\r\n\r\n        # adjacency matrix is needed for train_data only, as GAT is trained for\r\n        # training data\r\n        self.validation_triples = validation_data[0]\r\n        self.test_triples = test_data[0]\r\n\r\n        self.headTailSelector = headTailSelector  # for selecting random entities\r\n        self.entity2id = entity2id\r\n        self.id2entity = {v: k for k, v in self.entity2id.items()}\r\n        self.relation2id = relation2id\r\n        self.id2relation = {v: k for k, v in self.relation2id.items()}\r\n        self.batch_size = batch_size\r\n        # ratio of valid to invalid samples per batch for training ConvKB Model\r\n        self.invalid_valid_ratio = int(valid_to_invalid_samples_ratio)\r\n\r\n        if(get_2hop):\r\n            self.graph = self.get_graph()\r\n            self.node_neighbors_2hop = self.get_further_neighbors()\r\n\r\n        self.unique_entities_train = [self.entity2id[i]\r\n                                      for i in unique_entities_train]\r\n\r\n        self.train_indices = np.array(\r\n            list(self.train_triples)).astype(np.int32)\r\n        # These are valid triples, hence all have value 1\r\n        self.train_values = np.array(\r\n            [[1]] * len(self.train_triples)).astype(np.float32)\r\n\r\n        self.validation_indices = np.array(\r\n            list(self.validation_triples)).astype(np.int32)\r\n        self.validation_values = np.array(\r\n            [[1]] * len(self.validation_triples)).astype(np.float32)\r\n\r\n        self.test_indices = np.array(list(self.test_triples)).astype(np.int32)\r\n        self.test_values = np.array(\r\n            [[1]] * len(self.test_triples)).astype(np.float32)\r\n\r\n        self.valid_triples_dict = {j: i for i, j in enumerate(\r\n            self.train_triples + self.validation_triples + self.test_triples)}\r\n        print(\"Total triples count {}, training triples {}, validation_triples {}, test_triples {}\".format(len(self.valid_triples_dict), len(self.train_indices),\r\n                                                                                                           len(self.validation_indices), len(self.test_indices)))\r\n\r\n        # For training purpose\r\n        self.batch_indices = np.empty(\r\n            (self.batch_size * (self.invalid_valid_ratio + 1), 3)).astype(np.int32)\r\n        self.batch_values = np.empty(\r\n            (self.batch_size * (self.invalid_valid_ratio + 1), 1)).astype(np.float32)\r\n\r\n    def get_iteration_batch(self, iter_num):\r\n        if (iter_num + 1) * self.batch_size <= len(self.train_indices):\r\n            self.batch_indices = np.empty(\r\n                (self.batch_size * (self.invalid_valid_ratio + 1), 3)).astype(np.int32)\r\n            self.batch_values = np.empty(\r\n                (self.batch_size * (self.invalid_valid_ratio + 1), 1)).astype(np.float32)\r\n\r\n            indices = range(self.batch_size * iter_num,\r\n                            self.batch_size * (iter_num + 1))\r\n\r\n            self.batch_indices[:self.batch_size,\r\n                               :] = self.train_indices[indices, :]\r\n            self.batch_values[:self.batch_size,\r\n                              :] = self.train_values[indices, :]\r\n\r\n            last_index = self.batch_size\r\n\r\n            if self.invalid_valid_ratio > 0:\r\n                random_entities = np.random.randint(\r\n                    0, len(self.entity2id), last_index * self.invalid_valid_ratio)\r\n\r\n                # Precopying the same valid indices from 0 to batch_size to rest\r\n                # of the indices\r\n                self.batch_indices[last_index:(last_index * (self.invalid_valid_ratio + 1)), :] = np.tile(\r\n                    self.batch_indices[:last_index, :], (self.invalid_valid_ratio, 1))\r\n                self.batch_values[last_index:(last_index * (self.invalid_valid_ratio + 1)), :] = np.tile(\r\n                    self.batch_values[:last_index, :], (self.invalid_valid_ratio, 1))\r\n\r\n                for i in range(last_index):\r\n                    for j in range(self.invalid_valid_ratio // 2):\r\n                        current_index = i * (self.invalid_valid_ratio // 2) + j\r\n\r\n                        while (random_entities[current_index], self.batch_indices[last_index + current_index, 1],\r\n                               self.batch_indices[last_index + current_index, 2]) in self.valid_triples_dict.keys():\r\n                            random_entities[current_index] = np.random.randint(\r\n                                0, len(self.entity2id))\r\n                        self.batch_indices[last_index + current_index,\r\n                                           0] = random_entities[current_index]\r\n                        self.batch_values[last_index + current_index, :] = [-1]\r\n\r\n                    for j in range(self.invalid_valid_ratio // 2):\r\n                        current_index = last_index * \\\r\n                            (self.invalid_valid_ratio // 2) + \\\r\n                            (i * (self.invalid_valid_ratio // 2) + j)\r\n\r\n                        while (self.batch_indices[last_index + current_index, 0], self.batch_indices[last_index + current_index, 1],\r\n                               random_entities[current_index]) in self.valid_triples_dict.keys():\r\n                            random_entities[current_index] = np.random.randint(\r\n                                0, len(self.entity2id))\r\n                        self.batch_indices[last_index + current_index,\r\n                                           2] = random_entities[current_index]\r\n                        self.batch_values[last_index + current_index, :] = [-1]\r\n\r\n                return self.batch_indices, self.batch_values\r\n\r\n            return self.batch_indices, self.batch_values\r\n\r\n        else:\r\n            last_iter_size = len(self.train_indices) - \\\r\n                self.batch_size * iter_num\r\n            self.batch_indices = np.empty(\r\n                (last_iter_size * (self.invalid_valid_ratio + 1), 3)).astype(np.int32)\r\n            self.batch_values = np.empty(\r\n                (last_iter_size * (self.invalid_valid_ratio + 1), 1)).astype(np.float32)\r\n\r\n            indices = range(self.batch_size * iter_num,\r\n                            len(self.train_indices))\r\n            self.batch_indices[:last_iter_size,\r\n                               :] = self.train_indices[indices, :]\r\n            self.batch_values[:last_iter_size,\r\n                              :] = self.train_values[indices, :]\r\n\r\n            last_index = last_iter_size\r\n\r\n            if self.invalid_valid_ratio > 0:\r\n                random_entities = np.random.randint(\r\n                    0, len(self.entity2id), last_index * self.invalid_valid_ratio)\r\n\r\n                # Precopying the same valid indices from 0 to batch_size to rest\r\n                # of the indices\r\n                self.batch_indices[last_index:(last_index * (self.invalid_valid_ratio + 1)), :] = np.tile(\r\n                    self.batch_indices[:last_index, :], (self.invalid_valid_ratio, 1))\r\n                self.batch_values[last_index:(last_index * (self.invalid_valid_ratio + 1)), :] = np.tile(\r\n                    self.batch_values[:last_index, :], (self.invalid_valid_ratio, 1))\r\n\r\n                for i in range(last_index):\r\n                    for j in range(self.invalid_valid_ratio // 2):\r\n                        current_index = i * (self.invalid_valid_ratio // 2) + j\r\n\r\n                        while (random_entities[current_index], self.batch_indices[last_index + current_index, 1],\r\n                               self.batch_indices[last_index + current_index, 2]) in self.valid_triples_dict.keys():\r\n                            random_entities[current_index] = np.random.randint(\r\n                                0, len(self.entity2id))\r\n                        self.batch_indices[last_index + current_index,\r\n                                           0] = random_entities[current_index]\r\n                        self.batch_values[last_index + current_index, :] = [-1]\r\n\r\n                    for j in range(self.invalid_valid_ratio // 2):\r\n                        current_index = last_index * \\\r\n                            (self.invalid_valid_ratio // 2) + \\\r\n                            (i * (self.invalid_valid_ratio // 2) + j)\r\n\r\n                        while (self.batch_indices[last_index + current_index, 0], self.batch_indices[last_index + current_index, 1],\r\n                               random_entities[current_index]) in self.valid_triples_dict.keys():\r\n                            random_entities[current_index] = np.random.randint(\r\n                                0, len(self.entity2id))\r\n                        self.batch_indices[last_index + current_index,\r\n                                           2] = random_entities[current_index]\r\n                        self.batch_values[last_index + current_index, :] = [-1]\r\n\r\n                return self.batch_indices, self.batch_values\r\n\r\n            return self.batch_indices, self.batch_values\r\n\r\n    def get_iteration_batch_nhop(self, current_batch_indices, node_neighbors, batch_size):\r\n\r\n        self.batch_indices = np.empty(\r\n            (batch_size * (self.invalid_valid_ratio + 1), 4)).astype(np.int32)\r\n        self.batch_values = np.empty(\r\n            (batch_size * (self.invalid_valid_ratio + 1), 1)).astype(np.float32)\r\n        indices = random.sample(range(len(current_batch_indices)), batch_size)\r\n\r\n        self.batch_indices[:batch_size,\r\n                           :] = current_batch_indices[indices, :]\r\n        self.batch_values[:batch_size,\r\n                          :] = np.ones((batch_size, 1))\r\n\r\n        last_index = batch_size\r\n\r\n        if self.invalid_valid_ratio > 0:\r\n            random_entities = np.random.randint(\r\n                0, len(self.entity2id), last_index * self.invalid_valid_ratio)\r\n\r\n            # Precopying the same valid indices from 0 to batch_size to rest\r\n            # of the indices\r\n            self.batch_indices[last_index:(last_index * (self.invalid_valid_ratio + 1)), :] = np.tile(\r\n                self.batch_indices[:last_index, :], (self.invalid_valid_ratio, 1))\r\n            self.batch_values[last_index:(last_index * (self.invalid_valid_ratio + 1)), :] = np.tile(\r\n                self.batch_values[:last_index, :], (self.invalid_valid_ratio, 1))\r\n\r\n            for i in range(last_index):\r\n                for j in range(self.invalid_valid_ratio // 2):\r\n                    current_index = i * (self.invalid_valid_ratio // 2) + j\r\n\r\n                    self.batch_indices[last_index + current_index,\r\n                                       0] = random_entities[current_index]\r\n                    self.batch_values[last_index + current_index, :] = [0]\r\n\r\n                for j in range(self.invalid_valid_ratio // 2):\r\n                    current_index = last_index * \\\r\n                        (self.invalid_valid_ratio // 2) + \\\r\n                        (i * (self.invalid_valid_ratio // 2) + j)\r\n\r\n                    self.batch_indices[last_index + current_index,\r\n                                       3] = random_entities[current_index]\r\n                    self.batch_values[last_index + current_index, :] = [0]\r\n\r\n            return self.batch_indices, self.batch_values\r\n\r\n        return self.batch_indices, self.batch_values\r\n\r\n    # Hàm tạo một hash dictionary có cấu trúc graph[head][tail] = value\r\n    def get_graph(self):\r\n        graph = {}\r\n        all_tiples = torch.cat([self.train_adj_matrix[0].transpose(\r\n            0, 1), self.train_adj_matrix[1].unsqueeze(1)], dim=1)\r\n\r\n        for data in all_tiples:\r\n            source = data[1].data.item()\r\n            target = data[0].data.item()\r\n            value = data[2].data.item()\r\n\r\n            if(source not in graph.keys()):\r\n                graph[source] = {}\r\n                graph[source][target] = value\r\n            else:\r\n                graph[source][target] = value\r\n        print(\"Graph created\")\r\n        return graph\r\n\r\n    # Trả ra danh sách neighbors[distance] = [(tuple(relations), tuple(entities[:-1]))]\r\n    def bfs(self, graph, source, nbd_size=2):\r\n        visit = {}\r\n        distance = {}\r\n        parent = {}\r\n        distance_lengths = {}\r\n\r\n        visit[source] = 1\r\n        distance[source] = 0\r\n        parent[source] = (-1, -1)\r\n\r\n        q = queue.Queue()\r\n        q.put((source, -1))\r\n\r\n        while(not q.empty()):\r\n            top = q.get()\r\n            if top[0] in graph.keys():\r\n                for target in graph[top[0]].keys():\r\n                    if(target in visit.keys()):\r\n                        continue\r\n                    else:\r\n                        q.put((target, graph[top[0]][target]))\r\n\r\n                        distance[target] = distance[top[0]] + 1\r\n\r\n                        visit[target] = 1\r\n                        if distance[target] > 2:\r\n                            continue\r\n                        parent[target] = (top[0], graph[top[0]][target])\r\n\r\n                        if distance[target] not in distance_lengths.keys():\r\n                            distance_lengths[distance[target]] = 1\r\n\r\n        neighbors = {}\r\n        for target in visit.keys():\r\n            if(distance[target] != nbd_size):\r\n                continue\r\n            edges = [-1, parent[target][1]]\r\n            relations = []\r\n            entities = [target]\r\n            temp = target\r\n            while(parent[temp] != (-1, -1)):\r\n                relations.append(parent[temp][1])\r\n                entities.append(parent[temp][0])\r\n                temp = parent[temp][0]\r\n\r\n            if(distance[target] in neighbors.keys()):\r\n                neighbors[distance[target]].append(\r\n                    (tuple(relations), tuple(entities[:-1])))\r\n            else:\r\n                neighbors[distance[target]] = [\r\n                    (tuple(relations), tuple(entities[:-1]))]\r\n\r\n        return neighbors\r\n\r\n    # Trả ra các node kế cận neighbors[source][distance] = [(tuple(relations), tuple(entities[:-1]))]\r\n    # entities[:-1] là lấy các entity kế cận trừ phần tử nguồn\r\n    # Tìm tất cả các node bắt đầu từ source, có khoảng cách distance=[1,2] node kế cận đi qua các realtions và entities\r\n    def get_further_neighbors(self, nbd_size=2):\r\n        neighbors = {}\r\n        start_time = time.time()\r\n        print(\"length of graph keys is \", len(self.graph.keys()))\r\n        for source in self.graph.keys():\r\n            # st_time = time.time()\r\n            temp_neighbors = self.bfs(self.graph, source, nbd_size)\r\n            for distance in temp_neighbors.keys():\r\n                if(source in neighbors.keys()):\r\n                    if(distance in neighbors[source].keys()):\r\n                        neighbors[source][distance].append(\r\n                            temp_neighbors[distance])\r\n                    else:\r\n                        neighbors[source][distance] = temp_neighbors[distance]\r\n                else:\r\n                    neighbors[source] = {}\r\n                    neighbors[source][distance] = temp_neighbors[distance]\r\n\r\n        print(\"time taken \", time.time() - start_time)\r\n\r\n        print(\"length of neighbors dict is \", len(neighbors))\r\n        return neighbors\r\n\r\n    # Trả ra tất các các node kế cận của một batch_sources\r\n    def get_batch_nhop_neighbors_all(self, args, batch_sources, node_neighbors, nbd_size=2):\r\n        batch_source_triples = []\r\n        print(\"length of unique_entities \", len(batch_sources))\r\n        count = 0\r\n        for source in batch_sources:\r\n            # randomly select from the list of neighbors\r\n            if source in node_neighbors.keys():\r\n                nhop_list = node_neighbors[source][nbd_size] # nbd_size ~ distance = 1, 2\r\n\r\n                for i, tup in enumerate(nhop_list):\r\n                    if(args.partial_2hop and i >= 2):\r\n                        break\r\n\r\n                    count += 1\r\n                    batch_source_triples.append([source, nhop_list[i][0][-1], nhop_list[i][0][0],\r\n                                                 nhop_list[i][1][0]]) # \r\n\r\n        return np.array(batch_source_triples).astype(np.int32)\r\n\r\n    def transe_scoring(self, batch_inputs, entity_embeddings, relation_embeddings):\r\n        source_embeds = entity_embeddings[batch_inputs[:, 0]]\r\n        relation_embeds = relation_embeddings[batch_inputs[:, 1]]\r\n        tail_embeds = entity_embeddings[batch_inputs[:, 2]]\r\n        x = source_embeds + relation_embed - tail_embeds\r\n        x = torch.norm(x, p=1, dim=1)\r\n        return x\r\n\r\n    def get_validation_pred(self, args, model, unique_entities):\r\n        average_hits_at_100_head, average_hits_at_100_tail = [], []\r\n        average_hits_at_ten_head, average_hits_at_ten_tail = [], []\r\n        average_hits_at_three_head, average_hits_at_three_tail = [], []\r\n        average_hits_at_one_head, average_hits_at_one_tail = [], []\r\n        average_mean_rank_head, average_mean_rank_tail = [], []\r\n        average_mean_recip_rank_head, average_mean_recip_rank_tail = [], []\r\n\r\n        for iters in range(1):\r\n            start_time = time.time()\r\n\r\n            indices = [i for i in range(len(self.test_indices))]\r\n            batch_indices = self.test_indices[indices, :]\r\n            print(\"Sampled indices\")\r\n            print(\"test set length \", len(self.test_indices))\r\n            entity_list = [j for i, j in self.entity2id.items()]\r\n\r\n            ranks_head, ranks_tail = [], []\r\n            reciprocal_ranks_head, reciprocal_ranks_tail = [], []\r\n            hits_at_100_head, hits_at_100_tail = 0, 0\r\n            hits_at_ten_head, hits_at_ten_tail = 0, 0\r\n            hits_at_three_head, hits_at_three_tail = 0, 0\r\n            hits_at_one_head, hits_at_one_tail = 0, 0\r\n\r\n            for i in range(batch_indices.shape[0]):\r\n                print(len(ranks_head))\r\n                start_time_it = time.time()\r\n                new_x_batch_head = np.tile(\r\n                    batch_indices[i, :], (len(self.entity2id), 1))\r\n                new_x_batch_tail = np.tile(\r\n                    batch_indices[i, :], (len(self.entity2id), 1))\r\n\r\n                if(batch_indices[i, 0] not in unique_entities or batch_indices[i, 2] not in unique_entities):\r\n                    continue\r\n\r\n                new_x_batch_head[:, 0] = entity_list\r\n                new_x_batch_tail[:, 2] = entity_list\r\n\r\n                last_index_head = []  # array of already existing triples\r\n                last_index_tail = []\r\n                for tmp_index in range(len(new_x_batch_head)):\r\n                    temp_triple_head = (new_x_batch_head[tmp_index][0], new_x_batch_head[tmp_index][1],\r\n                                        new_x_batch_head[tmp_index][2])\r\n                    if temp_triple_head in self.valid_triples_dict.keys():\r\n                        last_index_head.append(tmp_index)\r\n\r\n                    temp_triple_tail = (new_x_batch_tail[tmp_index][0], new_x_batch_tail[tmp_index][1],\r\n                                        new_x_batch_tail[tmp_index][2])\r\n                    if temp_triple_tail in self.valid_triples_dict.keys():\r\n                        last_index_tail.append(tmp_index)\r\n\r\n                # Deleting already existing triples, leftover triples are invalid, according\r\n                # to train, validation and test data\r\n                # Note, all of them maynot be actually invalid\r\n                new_x_batch_head = np.delete(\r\n                    new_x_batch_head, last_index_head, axis=0)\r\n                new_x_batch_tail = np.delete(\r\n                    new_x_batch_tail, last_index_tail, axis=0)\r\n\r\n                # adding the current valid triples to the top, i.e, index 0\r\n                new_x_batch_head = np.insert(\r\n                    new_x_batch_head, 0, batch_indices[i], axis=0)\r\n                new_x_batch_tail = np.insert(\r\n                    new_x_batch_tail, 0, batch_indices[i], axis=0)\r\n\r\n                import math\r\n                # Have to do this, because it doesn't fit in memory\r\n\r\n                if 'WN' in args.data:\r\n                    num_triples_each_shot = int(\r\n                        math.ceil(new_x_batch_head.shape[0] / 4))\r\n\r\n                    scores1_head = model.batch_test(torch.LongTensor(\r\n                        new_x_batch_head[:num_triples_each_shot, :]).cuda())\r\n                    scores2_head = model.batch_test(torch.LongTensor(\r\n                        new_x_batch_head[num_triples_each_shot: 2 * num_triples_each_shot, :]).cuda())\r\n                    scores3_head = model.batch_test(torch.LongTensor(\r\n                        new_x_batch_head[2 * num_triples_each_shot: 3 * num_triples_each_shot, :]).cuda())\r\n                    scores4_head = model.batch_test(torch.LongTensor(\r\n                        new_x_batch_head[3 * num_triples_each_shot: 4 * num_triples_each_shot, :]).cuda())\r\n                    # scores5_head = model.batch_test(torch.LongTensor(\r\n                    #     new_x_batch_head[4 * num_triples_each_shot: 5 * num_triples_each_shot, :]).cuda())\r\n                    # scores6_head = model.batch_test(torch.LongTensor(\r\n                    #     new_x_batch_head[5 * num_triples_each_shot: 6 * num_triples_each_shot, :]).cuda())\r\n                    # scores7_head = model.batch_test(torch.LongTensor(\r\n                    #     new_x_batch_head[6 * num_triples_each_shot: 7 * num_triples_each_shot, :]).cuda())\r\n                    # scores8_head = model.batch_test(torch.LongTensor(\r\n                    #     new_x_batch_head[7 * num_triples_each_shot: 8 * num_triples_each_shot, :]).cuda())\r\n                    # scores9_head = model.batch_test(torch.LongTensor(\r\n                    #     new_x_batch_head[8 * num_triples_each_shot: 9 * num_triples_each_shot, :]).cuda())\r\n                    # scores10_head = model.batch_test(torch.LongTensor(\r\n                    #     new_x_batch_head[9 * num_triples_each_shot:, :]).cuda())\r\n\r\n                    scores_head = torch.cat(\r\n                        [scores1_head, scores2_head, scores3_head, scores4_head], dim=0)\r\n                    #scores5_head, scores6_head, scores7_head, scores8_head,\r\n                    # cores9_head, scores10_head], dim=0)\r\n                else:\r\n                    scores_head = model.batch_test(new_x_batch_head)\r\n\r\n                sorted_scores_head, sorted_indices_head = torch.sort(\r\n                    scores_head.view(-1), dim=-1, descending=True)\r\n                # Just search for zeroth index in the sorted scores, we appended valid triple at top\r\n                ranks_head.append(\r\n                    np.where(sorted_indices_head.cpu().numpy() == 0)[0][0] + 1)\r\n                reciprocal_ranks_head.append(1.0 / ranks_head[-1])\r\n\r\n                # Tail part here\r\n\r\n                if 'WN' in args.data:\r\n                    num_triples_each_shot = int(\r\n                        math.ceil(new_x_batch_tail.shape[0] / 4))\r\n\r\n                    scores1_tail = model.batch_test(torch.LongTensor(\r\n                        new_x_batch_tail[:num_triples_each_shot, :]).cuda())\r\n                    scores2_tail = model.batch_test(torch.LongTensor(\r\n                        new_x_batch_tail[num_triples_each_shot: 2 * num_triples_each_shot, :]).cuda())\r\n                    scores3_tail = model.batch_test(torch.LongTensor(\r\n                        new_x_batch_tail[2 * num_triples_each_shot: 3 * num_triples_each_shot, :]).cuda())\r\n                    scores4_tail = model.batch_test(torch.LongTensor(\r\n                        new_x_batch_tail[3 * num_triples_each_shot: 4 * num_triples_each_shot, :]).cuda())\r\n                    # scores5_tail = model.batch_test(torch.LongTensor(\r\n                    #     new_x_batch_tail[4 * num_triples_each_shot: 5 * num_triples_each_shot, :]).cuda())\r\n                    # scores6_tail = model.batch_test(torch.LongTensor(\r\n                    #     new_x_batch_tail[5 * num_triples_each_shot: 6 * num_triples_each_shot, :]).cuda())\r\n                    # scores7_tail = model.batch_test(torch.LongTensor(\r\n                    #     new_x_batch_tail[6 * num_triples_each_shot: 7 * num_triples_each_shot, :]).cuda())\r\n                    # scores8_tail = model.batch_test(torch.LongTensor(\r\n                    #     new_x_batch_tail[7 * num_triples_each_shot: 8 * num_triples_each_shot, :]).cuda())\r\n                    # scores9_tail = model.batch_test(torch.LongTensor(\r\n                    #     new_x_batch_tail[8 * num_triples_each_shot: 9 * num_triples_each_shot, :]).cuda())\r\n                    # scores10_tail = model.batch_test(torch.LongTensor(\r\n                    #     new_x_batch_tail[9 * num_triples_each_shot:, :]).cuda())\r\n\r\n                    scores_tail = torch.cat(\r\n                        [scores1_tail, scores2_tail, scores3_tail, scores4_tail], dim=0)\r\n                    #     scores5_tail, scores6_tail, scores7_tail, scores8_tail,\r\n                    #     scores9_tail, scores10_tail], dim=0)\r\n\r\n                else:\r\n                    scores_tail = model.batch_test(new_x_batch_tail)\r\n\r\n                sorted_scores_tail, sorted_indices_tail = torch.sort(\r\n                    scores_tail.view(-1), dim=-1, descending=True)\r\n\r\n                # Just search for zeroth index in the sorted scores, we appended valid triple at top\r\n                ranks_tail.append(\r\n                    np.where(sorted_indices_tail.cpu().numpy() == 0)[0][0] + 1)\r\n                reciprocal_ranks_tail.append(1.0 / ranks_tail[-1])\r\n                print(\"sample - \", ranks_head[-1], ranks_tail[-1])\r\n\r\n            for i in range(len(ranks_head)):\r\n                if ranks_head[i] <= 100:\r\n                    hits_at_100_head = hits_at_100_head + 1\r\n                if ranks_head[i] <= 10:\r\n                    hits_at_ten_head = hits_at_ten_head + 1\r\n                if ranks_head[i] <= 3:\r\n                    hits_at_three_head = hits_at_three_head + 1\r\n                if ranks_head[i] == 1:\r\n                    hits_at_one_head = hits_at_one_head + 1\r\n\r\n            for i in range(len(ranks_tail)):\r\n                if ranks_tail[i] <= 100:\r\n                    hits_at_100_tail = hits_at_100_tail + 1\r\n                if ranks_tail[i] <= 10:\r\n                    hits_at_ten_tail = hits_at_ten_tail + 1\r\n                if ranks_tail[i] <= 3:\r\n                    hits_at_three_tail = hits_at_three_tail + 1\r\n                if ranks_tail[i] == 1:\r\n                    hits_at_one_tail = hits_at_one_tail + 1\r\n\r\n            assert len(ranks_head) == len(reciprocal_ranks_head)\r\n            assert len(ranks_tail) == len(reciprocal_ranks_tail)\r\n            print(\"here {}\".format(len(ranks_head)))\r\n            print(\"\\nCurrent iteration time {}\".format(time.time() - start_time))\r\n            print(\"Stats for replacing head are -> \")\r\n            print(\"Current iteration Hits@100 are {}\".format(\r\n                hits_at_100_head / float(len(ranks_head))))\r\n            print(\"Current iteration Hits@10 are {}\".format(\r\n                hits_at_ten_head / len(ranks_head)))\r\n            print(\"Current iteration Hits@3 are {}\".format(\r\n                hits_at_three_head / len(ranks_head)))\r\n            print(\"Current iteration Hits@1 are {}\".format(\r\n                hits_at_one_head / len(ranks_head)))\r\n            print(\"Current iteration Mean rank {}\".format(\r\n                sum(ranks_head) / len(ranks_head)))\r\n            print(\"Current iteration Mean Reciprocal Rank {}\".format(\r\n                sum(reciprocal_ranks_head) / len(reciprocal_ranks_head)))\r\n\r\n            print(\"\\nStats for replacing tail are -> \")\r\n            print(\"Current iteration Hits@100 are {}\".format(\r\n                hits_at_100_tail / len(ranks_head)))\r\n            print(\"Current iteration Hits@10 are {}\".format(\r\n                hits_at_ten_tail / len(ranks_head)))\r\n            print(\"Current iteration Hits@3 are {}\".format(\r\n                hits_at_three_tail / len(ranks_head)))\r\n            print(\"Current iteration Hits@1 are {}\".format(\r\n                hits_at_one_tail / len(ranks_head)))\r\n            print(\"Current iteration Mean rank {}\".format(\r\n                sum(ranks_tail) / len(ranks_tail)))\r\n            print(\"Current iteration Mean Reciprocal Rank {}\".format(\r\n                sum(reciprocal_ranks_tail) / len(reciprocal_ranks_tail)))\r\n\r\n            average_hits_at_100_head.append(\r\n                hits_at_100_head / len(ranks_head))\r\n            average_hits_at_ten_head.append(\r\n                hits_at_ten_head / len(ranks_head))\r\n            average_hits_at_three_head.append(\r\n                hits_at_three_head / len(ranks_head))\r\n            average_hits_at_one_head.append(\r\n                hits_at_one_head / len(ranks_head))\r\n            average_mean_rank_head.append(sum(ranks_head) / len(ranks_head))\r\n            average_mean_recip_rank_head.append(\r\n                sum(reciprocal_ranks_head) / len(reciprocal_ranks_head))\r\n\r\n            average_hits_at_100_tail.append(\r\n                hits_at_100_tail / len(ranks_head))\r\n            average_hits_at_ten_tail.append(\r\n                hits_at_ten_tail / len(ranks_head))\r\n            average_hits_at_three_tail.append(\r\n                hits_at_three_tail / len(ranks_head))\r\n            average_hits_at_one_tail.append(\r\n                hits_at_one_tail / len(ranks_head))\r\n            average_mean_rank_tail.append(sum(ranks_tail) / len(ranks_tail))\r\n            average_mean_recip_rank_tail.append(\r\n                sum(reciprocal_ranks_tail) / len(reciprocal_ranks_tail))\r\n\r\n        print(\"\\nAveraged stats for replacing head are -> \")\r\n        print(\"Hits@100 are {}\".format(\r\n            sum(average_hits_at_100_head) / len(average_hits_at_100_head)))\r\n        print(\"Hits@10 are {}\".format(\r\n            sum(average_hits_at_ten_head) / len(average_hits_at_ten_head)))\r\n        print(\"Hits@3 are {}\".format(\r\n            sum(average_hits_at_three_head) / len(average_hits_at_three_head)))\r\n        print(\"Hits@1 are {}\".format(\r\n            sum(average_hits_at_one_head) / len(average_hits_at_one_head)))\r\n        print(\"Mean rank {}\".format(\r\n            sum(average_mean_rank_head) / len(average_mean_rank_head)))\r\n        print(\"Mean Reciprocal Rank {}\".format(\r\n            sum(average_mean_recip_rank_head) / len(average_mean_recip_rank_head)))\r\n\r\n        print(\"\\nAveraged stats for replacing tail are -> \")\r\n        print(\"Hits@100 are {}\".format(\r\n            sum(average_hits_at_100_tail) / len(average_hits_at_100_tail)))\r\n        print(\"Hits@10 are {}\".format(\r\n            sum(average_hits_at_ten_tail) / len(average_hits_at_ten_tail)))\r\n        print(\"Hits@3 are {}\".format(\r\n            sum(average_hits_at_three_tail) / len(average_hits_at_three_tail)))\r\n        print(\"Hits@1 are {}\".format(\r\n            sum(average_hits_at_one_tail) / len(average_hits_at_one_tail)))\r\n        print(\"Mean rank {}\".format(\r\n            sum(average_mean_rank_tail) / len(average_mean_rank_tail)))\r\n        print(\"Mean Reciprocal Rank {}\".format(\r\n            sum(average_mean_recip_rank_tail) / len(average_mean_recip_rank_tail)))\r\n\r\n        cumulative_hits_100 = (sum(average_hits_at_100_head) / len(average_hits_at_100_head)\r\n                               + sum(average_hits_at_100_tail) / len(average_hits_at_100_tail)) / 2\r\n        cumulative_hits_ten = (sum(average_hits_at_ten_head) / len(average_hits_at_ten_head)\r\n                               + sum(average_hits_at_ten_tail) / len(average_hits_at_ten_tail)) / 2\r\n        cumulative_hits_three = (sum(average_hits_at_three_head) / len(average_hits_at_three_head)\r\n                                 + sum(average_hits_at_three_tail) / len(average_hits_at_three_tail)) / 2\r\n        cumulative_hits_one = (sum(average_hits_at_one_head) / len(average_hits_at_one_head)\r\n                               + sum(average_hits_at_one_tail) / len(average_hits_at_one_tail)) / 2\r\n        cumulative_mean_rank = (sum(average_mean_rank_head) / len(average_mean_rank_head)\r\n                                + sum(average_mean_rank_tail) / len(average_mean_rank_tail)) / 2\r\n        cumulative_mean_recip_rank = (sum(average_mean_recip_rank_head) / len(average_mean_recip_rank_head) + sum(\r\n            average_mean_recip_rank_tail) / len(average_mean_recip_rank_tail)) / 2\r\n\r\n        print(\"\\nCumulative stats are -> \")\r\n        print(\"Hits@100 are {}\".format(cumulative_hits_100))\r\n        print(\"Hits@10 are {}\".format(cumulative_hits_ten))\r\n        print(\"Hits@3 are {}\".format(cumulative_hits_three))\r\n        print(\"Hits@1 are {}\".format(cumulative_hits_one))\r\n        print(\"Mean rank {}\".format(cumulative_mean_rank))\r\n        print(\"Mean Reciprocal Rank {}\".format(cumulative_mean_recip_rank))\r\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- create_batch.py	(revision a7ee2c75c3a843e544ca707b61eb104788859d25)
+++ create_batch.py	(date 1596185318379)
@@ -333,7 +333,7 @@
         for source in batch_sources:
             # randomly select from the list of neighbors
             if source in node_neighbors.keys():
-                nhop_list = node_neighbors[source][nbd_size] # nbd_size ~ distance = 1, 2
+                nhop_list = node_neighbors[source][nbd_size] # nbd_size ~ distance = 2
 
                 for i, tup in enumerate(nhop_list):
                     if(args.partial_2hop and i >= 2):
@@ -426,13 +426,13 @@
                         math.ceil(new_x_batch_head.shape[0] / 4))
 
                     scores1_head = model.batch_test(torch.LongTensor(
-                        new_x_batch_head[:num_triples_each_shot, :]).cuda())
+                        new_x_batch_head[:num_triples_each_shot, :]))
                     scores2_head = model.batch_test(torch.LongTensor(
-                        new_x_batch_head[num_triples_each_shot: 2 * num_triples_each_shot, :]).cuda())
+                        new_x_batch_head[num_triples_each_shot: 2 * num_triples_each_shot, :]))
                     scores3_head = model.batch_test(torch.LongTensor(
-                        new_x_batch_head[2 * num_triples_each_shot: 3 * num_triples_each_shot, :]).cuda())
+                        new_x_batch_head[2 * num_triples_each_shot: 3 * num_triples_each_shot, :]))
                     scores4_head = model.batch_test(torch.LongTensor(
-                        new_x_batch_head[3 * num_triples_each_shot: 4 * num_triples_each_shot, :]).cuda())
+                        new_x_batch_head[3 * num_triples_each_shot: 4 * num_triples_each_shot, :]))
                     # scores5_head = model.batch_test(torch.LongTensor(
                     #     new_x_batch_head[4 * num_triples_each_shot: 5 * num_triples_each_shot, :]).cuda())
                     # scores6_head = model.batch_test(torch.LongTensor(
@@ -467,13 +467,13 @@
                         math.ceil(new_x_batch_tail.shape[0] / 4))
 
                     scores1_tail = model.batch_test(torch.LongTensor(
-                        new_x_batch_tail[:num_triples_each_shot, :]).cuda())
+                        new_x_batch_tail[:num_triples_each_shot, :]))
                     scores2_tail = model.batch_test(torch.LongTensor(
-                        new_x_batch_tail[num_triples_each_shot: 2 * num_triples_each_shot, :]).cuda())
+                        new_x_batch_tail[num_triples_each_shot: 2 * num_triples_each_shot, :]))
                     scores3_tail = model.batch_test(torch.LongTensor(
-                        new_x_batch_tail[2 * num_triples_each_shot: 3 * num_triples_each_shot, :]).cuda())
+                        new_x_batch_tail[2 * num_triples_each_shot: 3 * num_triples_each_shot, :]))
                     scores4_tail = model.batch_test(torch.LongTensor(
-                        new_x_batch_tail[3 * num_triples_each_shot: 4 * num_triples_each_shot, :]).cuda())
+                        new_x_batch_tail[3 * num_triples_each_shot: 4 * num_triples_each_shot, :]))
                     # scores5_tail = model.batch_test(torch.LongTensor(
                     #     new_x_batch_tail[4 * num_triples_each_shot: 5 * num_triples_each_shot, :]).cuda())
                     # scores6_tail = model.batch_test(torch.LongTensor(
Index: layers.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>import numpy as np\r\nimport torch\r\nimport torch.nn as nn\r\nimport time\r\nimport torch.nn.functional as F\r\nfrom torch.autograd import Variable\r\n\r\n\r\nCUDA = torch.cuda.is_available()\r\n\r\n\r\nclass ConvKB(nn.Module):\r\n    def __init__(self, input_dim, input_seq_len, in_channels, out_channels, drop_prob, alpha_leaky):\r\n        super().__init__()\r\n\r\n        self.conv_layer = nn.Conv2d(\r\n            in_channels, out_channels, (1, input_seq_len))  # kernel size -> 1*input_seq_length(i.e. 2)\r\n        self.dropout = nn.Dropout(drop_prob)\r\n        self.non_linearity = nn.ReLU()\r\n        self.fc_layer = nn.Linear((input_dim) * out_channels, 1)\r\n\r\n        nn.init.xavier_uniform_(self.fc_layer.weight, gain=1.414)\r\n        nn.init.xavier_uniform_(self.conv_layer.weight, gain=1.414)\r\n\r\n    def forward(self, conv_input):\r\n\r\n        batch_size, length, dim = conv_input.size()\r\n        # assuming inputs are of the form ->\r\n        conv_input = conv_input.transpose(1, 2)\r\n        # batch * length(which is 3 here -> entity,relation,entity) * dim\r\n        # To make tensor of size 4, where second dim is for input channels\r\n        conv_input = conv_input.unsqueeze(1)\r\n\r\n        out_conv = self.dropout(\r\n            self.non_linearity(self.conv_layer(conv_input)))\r\n\r\n        input_fc = out_conv.squeeze(-1).view(batch_size, -1)\r\n        output = self.fc_layer(input_fc)\r\n        return output\r\n\r\n\r\nclass SpecialSpmmFunctionFinal(torch.autograd.Function):\r\n    \"\"\"Special function for only sparse region backpropataion layer.\"\"\"\r\n    @staticmethod\r\n    def forward(ctx, edge, edge_w, N, E, out_features):\r\n        # assert indices.requires_grad == False\r\n        a = torch.sparse_coo_tensor(\r\n            edge, edge_w, torch.Size([N, N, out_features]))\r\n        b = torch.sparse.sum(a, dim=1)\r\n        ctx.N = b.shape[0]\r\n        ctx.outfeat = b.shape[1]\r\n        ctx.E = E\r\n        ctx.indices = a._indices()[0, :]\r\n\r\n        return b.to_dense()\r\n\r\n    @staticmethod\r\n    def backward(ctx, grad_output):\r\n        grad_values = None\r\n        if ctx.needs_input_grad[1]:\r\n            edge_sources = ctx.indices\r\n\r\n            if(CUDA):\r\n                edge_sources = edge_sources.cuda()\r\n\r\n            grad_values = grad_output[edge_sources]\r\n            # grad_values = grad_values.view(ctx.E, ctx.outfeat)\r\n            # print(\"Grad Outputs-> \", grad_output)\r\n            # print(\"Grad values-> \", grad_values)\r\n        return None, grad_values, None, None, None\r\n\r\n\r\nclass SpecialSpmmFinal(nn.Module):\r\n    def forward(self, edge, edge_w, N, E, out_features):\r\n        return SpecialSpmmFunctionFinal.apply(edge, edge_w, N, E, out_features)\r\n\r\n\r\nclass SpGraphAttentionLayer(nn.Module):\r\n    \"\"\"\r\n    Sparse version GAT layer, similar to https://arxiv.org/abs/1710.10903\r\n    \"\"\"\r\n\r\n    def __init__(self, num_nodes, in_features, out_features, nrela_dim, dropout, alpha, concat=True):\r\n        super(SpGraphAttentionLayer, self).__init__()\r\n        self.in_features = in_features\r\n        self.out_features = out_features\r\n        self.num_nodes = num_nodes\r\n        self.alpha = alpha\r\n        self.concat = concat\r\n        self.nrela_dim = nrela_dim\r\n\r\n        self.a = nn.Parameter(torch.zeros(\r\n            size=(out_features, 2 * in_features + nrela_dim)))\r\n        nn.init.xavier_normal_(self.a.data, gain=1.414)\r\n        self.a_2 = nn.Parameter(torch.zeros(size=(1, out_features)))\r\n        nn.init.xavier_normal_(self.a_2.data, gain=1.414)\r\n\r\n        self.dropout = nn.Dropout(dropout)\r\n        self.leakyrelu = nn.LeakyReLU(self.alpha)\r\n        self.special_spmm_final = SpecialSpmmFinal()\r\n\r\n    def forward(self, input, edge, edge_embed, edge_list_nhop, edge_embed_nhop):\r\n        N = input.size()[0]\r\n\r\n        # Self-attention on the nodes - Shared attention mechanism\r\n        edge = torch.cat((edge[:, :], edge_list_nhop[:, :]), dim=1)\r\n        edge_embed = torch.cat(\r\n            (edge_embed[:, :], edge_embed_nhop[:, :]), dim=0)\r\n\r\n        edge_h = torch.cat(\r\n            (input[edge[0, :], :], input[edge[1, :], :], edge_embed[:, :]), dim=1).t()\r\n        # edge_h: (2*in_dim + nrela_dim) x E\r\n\r\n        edge_m = self.a.mm(edge_h)\r\n        # edge_m: D * E\r\n\r\n        # to be checked later\r\n        powers = -self.leakyrelu(self.a_2.mm(edge_m).squeeze())\r\n        edge_e = torch.exp(powers).unsqueeze(1)\r\n        assert not torch.isnan(edge_e).any()\r\n        # edge_e: E\r\n\r\n        e_rowsum = self.special_spmm_final(\r\n            edge, edge_e, N, edge_e.shape[0], 1)\r\n        e_rowsum[e_rowsum == 0.0] = 1e-12\r\n\r\n        e_rowsum = e_rowsum\r\n        # e_rowsum: N x 1\r\n        edge_e = edge_e.squeeze(1)\r\n\r\n        edge_e = self.dropout(edge_e)\r\n        # edge_e: E\r\n\r\n        edge_w = (edge_e * edge_m).t()\r\n        # edge_w: E * D\r\n\r\n        h_prime = self.special_spmm_final(\r\n            edge, edge_w, N, edge_w.shape[0], self.out_features)\r\n\r\n        assert not torch.isnan(h_prime).any()\r\n        # h_prime: N x out\r\n        h_prime = h_prime.div(e_rowsum)\r\n        # h_prime: N x out\r\n\r\n        assert not torch.isnan(h_prime).any()\r\n        if self.concat:\r\n            # if this layer is not last layer,\r\n            return F.elu(h_prime)\r\n        else:\r\n            # if this layer is last layer,\r\n            return h_prime\r\n\r\n    def __repr__(self):\r\n        return self.__class__.__name__ + ' (' + str(self.in_features) + ' -> ' + str(self.out_features) + ')'\r\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- layers.py	(revision a7ee2c75c3a843e544ca707b61eb104788859d25)
+++ layers.py	(date 1597758569434)
@@ -15,6 +15,7 @@
 
         self.conv_layer = nn.Conv2d(
             in_channels, out_channels, (1, input_seq_len))  # kernel size -> 1*input_seq_length(i.e. 2)
+
         self.dropout = nn.Dropout(drop_prob)
         self.non_linearity = nn.ReLU()
         self.fc_layer = nn.Linear((input_dim) * out_channels, 1)
@@ -31,8 +32,10 @@
         # To make tensor of size 4, where second dim is for input channels
         conv_input = conv_input.unsqueeze(1)
 
-        out_conv = self.dropout(
-            self.non_linearity(self.conv_layer(conv_input)))
+        # (n_samples, channels, height, width)
+        conv_result = self.conv_layer(conv_input)
+        conv_non_linear = self.non_linearity(conv_result)
+        out_conv = self.dropout(conv_non_linear)
 
         input_fc = out_conv.squeeze(-1).view(batch_size, -1)
         output = self.fc_layer(input_fc)
@@ -88,7 +91,7 @@
         self.alpha = alpha
         self.concat = concat
         self.nrela_dim = nrela_dim
-
+    
         self.a = nn.Parameter(torch.zeros(
             size=(out_features, 2 * in_features + nrela_dim)))
         nn.init.xavier_normal_(self.a.data, gain=1.414)
@@ -112,7 +115,7 @@
         # edge_h: (2*in_dim + nrela_dim) x E
 
         edge_m = self.a.mm(edge_h)
-        # edge_m: D * E
+        # edge_m: D * E 
 
         # to be checked later
         powers = -self.leakyrelu(self.a_2.mm(edge_m).squeeze())
Index: models.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>import torch\r\nimport torch.nn as nn\r\nimport torch.nn.functional as F\r\nfrom torch.autograd import Variable\r\nimport time\r\nfrom layers import SpGraphAttentionLayer, ConvKB\r\n\r\nCUDA = torch.cuda.is_available()  # checking cuda availability\r\n\r\n\r\nclass SpGAT(nn.Module):\r\n    def __init__(self, num_nodes, nfeat, nhid, relation_dim, dropout, alpha, nheads):\r\n        \"\"\"\r\n            Sparse version of GAT\r\n            nfeat -> Entity Input Embedding dimensions\r\n            nhid  -> Entity Output Embedding dimensions\r\n            relation_dim -> Relation Embedding dimensions\r\n            num_nodes -> number of nodes in the Graph\r\n            nheads -> Used for Multihead attention\r\n\r\n        \"\"\"\r\n        super(SpGAT, self).__init__()\r\n        self.dropout = dropout\r\n        self.dropout_layer = nn.Dropout(self.dropout)\r\n        self.attentions = [SpGraphAttentionLayer(num_nodes, nfeat,\r\n                                                 nhid,\r\n                                                 relation_dim,\r\n                                                 dropout=dropout,\r\n                                                 alpha=alpha,\r\n                                                 concat=True)\r\n                           for _ in range(nheads)]\r\n\r\n        for i, attention in enumerate(self.attentions):\r\n            self.add_module('attention_{}'.format(i), attention)\r\n\r\n        # W matrix to convert h_input to h_output dimension\r\n        self.W = nn.Parameter(torch.zeros(size=(relation_dim, nheads * nhid)))\r\n        nn.init.xavier_uniform_(self.W.data, gain=1.414)\r\n\r\n        self.out_att = SpGraphAttentionLayer(num_nodes, nhid * nheads,\r\n                                             nheads * nhid, nheads * nhid,\r\n                                             dropout=dropout,\r\n                                             alpha=alpha,\r\n                                             concat=False\r\n                                             )\r\n\r\n    def forward(self, Corpus_, batch_inputs, entity_embeddings, relation_embed,\r\n                edge_list, edge_type, edge_embed, edge_list_nhop, edge_type_nhop):\r\n        x = entity_embeddings\r\n\r\n        edge_embed_nhop = relation_embed[\r\n            edge_type_nhop[:, 0]] + relation_embed[edge_type_nhop[:, 1]]\r\n\r\n        x = torch.cat([att(x, edge_list, edge_embed, edge_list_nhop, edge_embed_nhop)\r\n                       for att in self.attentions], dim=1)\r\n        x = self.dropout_layer(x)\r\n\r\n        out_relation_1 = relation_embed.mm(self.W)\r\n\r\n        edge_embed = out_relation_1[edge_type]\r\n        edge_embed_nhop = out_relation_1[\r\n            edge_type_nhop[:, 0]] + out_relation_1[edge_type_nhop[:, 1]]\r\n\r\n        x = F.elu(self.out_att(x, edge_list, edge_embed,\r\n                               edge_list_nhop, edge_embed_nhop))\r\n        return x, out_relation_1\r\n\r\n\r\nclass SpKBGATModified(nn.Module):\r\n    def __init__(self, initial_entity_emb, initial_relation_emb, entity_out_dim, relation_out_dim,\r\n                 drop_GAT, alpha, nheads_GAT):\r\n        '''Sparse version of KBGAT\r\n        entity_in_dim -> Entity Input Embedding dimensions\r\n        entity_out_dim  -> Entity Output Embedding dimensions, passed as a list\r\n        num_relation -> number of unique relations\r\n        relation_dim -> Relation Embedding dimensions\r\n        num_nodes -> number of nodes in the Graph\r\n        nheads_GAT -> Used for Multihead attention, passed as a list '''\r\n\r\n        super().__init__()\r\n\r\n        self.num_nodes = initial_entity_emb.shape[0]\r\n        self.entity_in_dim = initial_entity_emb.shape[1]\r\n        self.entity_out_dim_1 = entity_out_dim[0]\r\n        self.nheads_GAT_1 = nheads_GAT[0]\r\n        self.entity_out_dim_2 = entity_out_dim[1]\r\n        self.nheads_GAT_2 = nheads_GAT[1]\r\n\r\n        # Properties of Relations\r\n        self.num_relation = initial_relation_emb.shape[0]\r\n        self.relation_dim = initial_relation_emb.shape[1]\r\n        self.relation_out_dim_1 = relation_out_dim[0]\r\n\r\n        self.drop_GAT = drop_GAT\r\n        self.alpha = alpha      # For leaky relu\r\n\r\n        self.final_entity_embeddings = nn.Parameter(\r\n            torch.randn(self.num_nodes, self.entity_out_dim_1 * self.nheads_GAT_1))\r\n\r\n        self.final_relation_embeddings = nn.Parameter(\r\n            torch.randn(self.num_relation, self.entity_out_dim_1 * self.nheads_GAT_1))\r\n\r\n        self.entity_embeddings = nn.Parameter(initial_entity_emb)\r\n        self.relation_embeddings = nn.Parameter(initial_relation_emb)\r\n\r\n        self.sparse_gat_1 = SpGAT(self.num_nodes, self.entity_in_dim, self.entity_out_dim_1, self.relation_dim,\r\n                                  self.drop_GAT, self.alpha, self.nheads_GAT_1)\r\n\r\n        self.W_entities = nn.Parameter(torch.zeros(\r\n            size=(self.entity_in_dim, self.entity_out_dim_1 * self.nheads_GAT_1)))\r\n        nn.init.xavier_uniform_(self.W_entities.data, gain=1.414)\r\n\r\n    def forward(self, Corpus_, adj, batch_inputs, train_indices_nhop):\r\n        # getting edge list\r\n        edge_list = adj[0] # head_id and tail_id\r\n        edge_type = adj[1] # relation_id\r\n\r\n        edge_list_nhop = torch.cat(\r\n            (train_indices_nhop[:, 3].unsqueeze(-1), train_indices_nhop[:, 0].unsqueeze(-1)), dim=1).t()\r\n        edge_type_nhop = torch.cat(\r\n            [train_indices_nhop[:, 1].unsqueeze(-1), train_indices_nhop[:, 2].unsqueeze(-1)], dim=1)\r\n\r\n        if(CUDA):\r\n            edge_list = edge_list.cuda()\r\n            edge_type = edge_type.cuda()\r\n            edge_list_nhop = edge_list_nhop.cuda()\r\n            edge_type_nhop = edge_type_nhop.cuda()\r\n\r\n        edge_embed = self.relation_embeddings[edge_type]\r\n\r\n        start = time.time()\r\n\r\n        self.entity_embeddings.data = F.normalize(\r\n            self.entity_embeddings.data, p=2, dim=1).detach()\r\n\r\n        # self.relation_embeddings.data = F.normalize(\r\n        #     self.relation_embeddings.data, p=2, dim=1)\r\n\r\n        out_entity_1, out_relation_1 = self.sparse_gat_1(\r\n            Corpus_, batch_inputs, self.entity_embeddings, self.relation_embeddings,\r\n            edge_list, edge_type, edge_embed, edge_list_nhop, edge_type_nhop)\r\n\r\n        if (CUDA):\r\n            mask_indices = torch.unique(batch_inputs[:, 2]).cuda()\r\n            mask = torch.zeros(self.entity_embeddings.shape[0]).cuda()\r\n        else:\r\n            mask_indices = torch.unique(batch_inputs[:, 2])\r\n            mask = torch.zeros(self.entity_embeddings.shape[0])\r\n        mask[mask_indices] = 1.0\r\n\r\n        entities_upgraded = self.entity_embeddings.mm(self.W_entities)\r\n        out_entity_1 = entities_upgraded + \\\r\n            mask.unsqueeze(-1).expand_as(out_entity_1) * out_entity_1\r\n\r\n        out_entity_1 = F.normalize(out_entity_1, p=2, dim=1)\r\n\r\n        self.final_entity_embeddings.data = out_entity_1.data\r\n        self.final_relation_embeddings.data = out_relation_1.data\r\n\r\n        return out_entity_1, out_relation_1\r\n\r\n\r\nclass SpKBGATConvOnly(nn.Module):\r\n    def __init__(self, initial_entity_emb, initial_relation_emb, entity_out_dim, relation_out_dim,\r\n                 drop_GAT, drop_conv, alpha, alpha_conv, nheads_GAT, conv_out_channels):\r\n        '''Sparse version of KBGAT\r\n        entity_in_dim -> Entity Input Embedding dimensions\r\n        entity_out_dim  -> Entity Output Embedding dimensions, passed as a list\r\n        num_relation -> number of unique relations\r\n        relation_dim -> Relation Embedding dimensions\r\n        num_nodes -> number of nodes in the Graph\r\n        nheads_GAT -> Used for Multihead attention, passed as a list '''\r\n\r\n        super().__init__()\r\n\r\n        self.num_nodes = initial_entity_emb.shape[0]\r\n        self.entity_in_dim = initial_entity_emb.shape[1]\r\n        self.entity_out_dim_1 = entity_out_dim[0]\r\n        self.nheads_GAT_1 = nheads_GAT[0]\r\n        self.entity_out_dim_2 = entity_out_dim[1]\r\n        self.nheads_GAT_2 = nheads_GAT[1]\r\n\r\n        # Properties of Relations\r\n        self.num_relation = initial_relation_emb.shape[0]\r\n        self.relation_dim = initial_relation_emb.shape[1]\r\n        self.relation_out_dim_1 = relation_out_dim[0]\r\n\r\n        self.drop_GAT = drop_GAT\r\n        self.drop_conv = drop_conv\r\n        self.alpha = alpha      # For leaky relu\r\n        self.alpha_conv = alpha_conv\r\n        self.conv_out_channels = conv_out_channels\r\n\r\n        self.final_entity_embeddings = nn.Parameter(\r\n            torch.randn(self.num_nodes, self.entity_out_dim_1 * self.nheads_GAT_1))\r\n\r\n        self.final_relation_embeddings = nn.Parameter(\r\n            torch.randn(self.num_relation, self.entity_out_dim_1 * self.nheads_GAT_1))\r\n\r\n        self.convKB = ConvKB(self.entity_out_dim_1 * self.nheads_GAT_1, 3, 1,\r\n                             self.conv_out_channels, self.drop_conv, self.alpha_conv)\r\n\r\n    def forward(self, Corpus_, adj, batch_inputs):\r\n        conv_input = torch.cat((self.final_entity_embeddings[batch_inputs[:, 0], :].unsqueeze(1), self.final_relation_embeddings[\r\n            batch_inputs[:, 1]].unsqueeze(1), self.final_entity_embeddings[batch_inputs[:, 2], :].unsqueeze(1)), dim=1)\r\n        out_conv = self.convKB(conv_input)\r\n        return out_conv\r\n\r\n    def batch_test(self, batch_inputs):\r\n        conv_input = torch.cat((self.final_entity_embeddings[batch_inputs[:, 0], :].unsqueeze(1), self.final_relation_embeddings[\r\n            batch_inputs[:, 1]].unsqueeze(1), self.final_entity_embeddings[batch_inputs[:, 2], :].unsqueeze(1)), dim=1)\r\n        out_conv = self.convKB(conv_input)\r\n        return out_conv\r\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- models.py	(revision a7ee2c75c3a843e544ca707b61eb104788859d25)
+++ models.py	(date 1598586194299)
@@ -38,7 +38,8 @@
         nn.init.xavier_uniform_(self.W.data, gain=1.414)
 
         self.out_att = SpGraphAttentionLayer(num_nodes, nhid * nheads,
-                                             nheads * nhid, nheads * nhid,
+                                             nheads * nhid,
+                                             nheads * nhid,
                                              dropout=dropout,
                                              alpha=alpha,
                                              concat=False
@@ -112,7 +113,7 @@
 
     def forward(self, Corpus_, adj, batch_inputs, train_indices_nhop):
         # getting edge list
-        edge_list = adj[0] # head_id and tail_id
+        edge_list = adj[0] # tail_id and head_id
         edge_type = adj[1] # relation_id
 
         edge_list_nhop = torch.cat(
